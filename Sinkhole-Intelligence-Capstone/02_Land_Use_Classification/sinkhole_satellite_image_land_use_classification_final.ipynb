{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4633d575",
   "metadata": {
    "id": "cace261f"
   },
   "source": [
    "# Land-Use Image Classification\n",
    "---\n",
    "Created by Carlos Moreno, Elizabeth Khan, Frances Leung, Jeffrey Laughman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe38069c",
   "metadata": {},
   "source": [
    "### Download dataset split into Train, Test, and Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c562cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install split-folders\n",
    "#! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa00d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "import wget\n",
    "import zipfile\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02cb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "# Make folder if does not exist\n",
    "if os.path.exists('sinkhole')==False:\n",
    "    os.makedirs('sinkhole')\n",
    "    os.chdir('sinkhole')\n",
    "else:\n",
    "    os.chdir('sinkhole')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcccc80",
   "metadata": {},
   "source": [
    "Download file and save to sinkhole folder location from https://madm.dfki.de/files/sentinel/EuroSAT.zip\n",
    "\n",
    "\n",
    "#### Helper function to unzip and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf6ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(new_folder='sinkhole'):\n",
    "    # create deep learning folder if does not exist\n",
    "#     if os.path.exists(new_folder)==False:\n",
    "#         os.makedirs(new_folder)\n",
    "#         os.chdir(new_folder)\n",
    "#     else:\n",
    "#         os.chdir(new_folder)\n",
    "    # download data\n",
    "    #requests.get('https://madm.dfki.de/files/sentinel/EuroSAT.zip')\n",
    "    # unzip file\n",
    "    with zipfile.ZipFile('EuroSAT.zip') as z:\n",
    "        z.extractall()  \n",
    "    # This will split folders into train, validation and test inside the output folder\n",
    "    splitfolders.ratio('2750','data')\n",
    "    \n",
    "    print('EuroSAT RGB files were successfully split, see data folder for train, test, and validation data')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64bc4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7baf4c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliza\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\eliza\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import gc \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdc1512d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear out cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "# garbage collection\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c888baed",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is tagged `parameters` for papermill\n",
    "LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "EPOCHS = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec40e9",
   "metadata": {
    "id": "c8ec40e9"
   },
   "source": [
    "### Set the architecture and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97b1a0aa",
   "metadata": {
    "id": "97b1a0aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\train \n",
      " data\\val\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Using the pre-trained resnet50 architecture with pretrained weights for transfer learning\n",
    "ARCH = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze all layers except for the final layer\n",
    "for param in ARCH.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "classes = ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture','PermanentCrop','Residential','River', 'SeaLake']\n",
    "\n",
    "# Getting the final layer to match the number of classes\n",
    "num_ftrs = ARCH.fc.in_features\n",
    "ARCH.fc = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "##########################\n",
    "\n",
    "##########################\n",
    "SEED=1\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "cudnn.deterministic = True\n",
    "#########################\n",
    "# Define epochs and printing frequency\n",
    "START_EPOCH = 0\n",
    "#EPOCHS = 10\n",
    "PRINT_FREQ = 2500\n",
    "\n",
    "#Dynamically set batch size and workers\n",
    "avail_gpus = min(1, torch.cuda.device_count())\n",
    "batch_size = 100 if avail_gpus else 64\n",
    "#batch_size = 50\n",
    "TRAIN_BATCH= batch_size\n",
    "VAL_BATCH = batch_size\n",
    "WORKERS= int(os.cpu_count()-2/2)\n",
    "\n",
    "# Files locations of training and validation data\n",
    "root_dir = \"data\"\n",
    "TRAINDIR= os.path.join(root_dir,'train')\n",
    "VALDIR= os.path.join(root_dir,'val')\n",
    "\n",
    "print(TRAINDIR,'\\n', VALDIR)\n",
    "\n",
    "# check if cuda is available in this cell\n",
    "# if it is not available, you should not go forward!\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# enable algorithm optimization\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdd6e12",
   "metadata": {
    "id": "7cdd6e12"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70dbd4c4",
   "metadata": {
    "id": "70dbd4c4"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    ######################\n",
    "    # switch model to train mode here\n",
    "    model.train()\n",
    "    ################\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        #####################\n",
    "        # send the images to cuda device\n",
    "        # send the target to cuda device\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "\n",
    "        # compute loss \n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        \n",
    "        #### zero out gradients in the optimier\n",
    "        optimizer.zero_grad() # Zero gradients each step \n",
    "        ## backprop!\n",
    "        loss.backward()\n",
    "        # update the weights!\n",
    "        optimizer.step()\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3b62e",
   "metadata": {
    "id": "deb3b62e"
   },
   "source": [
    "### Validate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f276cc",
   "metadata": {
    "id": "a3f276cc"
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    # model ???\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            \n",
    "            \n",
    "            ### send the images and target to cuda\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            # output = model ??? images?\n",
    "            output = model(images)\n",
    "            # compute loss\n",
    "            # loss  = criterion ?? output ?? target\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % PRINT_FREQ == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe49226",
   "metadata": {
    "id": "fbe49226"
   },
   "source": [
    "### Save the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff8a4159",
   "metadata": {
    "id": "ff8a4159"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    # save the model state!\n",
    "    #checkpoint = 'checkpoint'\n",
    "    filepath = filename\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cd7ea3a",
   "metadata": {
    "id": "1cd7ea3a"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1da87ab",
   "metadata": {
    "id": "e1da87ab"
   },
   "outputs": [],
   "source": [
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00211030",
   "metadata": {
    "id": "00211030"
   },
   "outputs": [],
   "source": [
    "# if we are adjusting the LR manually use this\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = LR * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da2c1382",
   "metadata": {
    "id": "da2c1382"
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c29e7a1",
   "metadata": {
    "id": "5c29e7a1"
   },
   "outputs": [],
   "source": [
    "# Use imagenet related mean and standard deviation for normalization\n",
    "imagenet_mean_RGB = [0.47889522, 0.47227842, 0.43047404]\n",
    "imagenet_std_RGB = [0.229, 0.224, 0.225]\n",
    "# cinic_mean_RGB = [0.47889522, 0.47227842, 0.43047404]\n",
    "# cinic_std_RGB = [0.24205776, 0.23828046, 0.25874835]\n",
    "# cifar_mean_RGB = [0.4914, 0.4822, 0.4465]\n",
    "# cifar_std_RGB = [0.2023, 0.1994, 0.2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1a61c02",
   "metadata": {
    "id": "e1a61c02"
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=imagenet_mean_RGB, std=imagenet_std_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47dd3e49",
   "metadata": {
    "id": "47dd3e49"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "IMG_SIZE2 = int(round(1.15 * IMG_SIZE,0))\n",
    "#IMG_SIZE = 224 #ALEXNET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4387cb",
   "metadata": {
    "id": "de4387cb"
   },
   "source": [
    "### Initialize the model using the architecture you selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1abcc33",
   "metadata": {
    "id": "b1abcc33"
   },
   "outputs": [],
   "source": [
    "# model = ... \n",
    "\n",
    "model = ARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1bb69",
   "metadata": {
    "id": "2db1bb69"
   },
   "source": [
    "### Send the model to the cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d23ccb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d23ccb4",
    "outputId": "3ba21fd9-922c-46c1-be03-0b4336e832d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send the model to the cuda device.. \n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8eb8d",
   "metadata": {
    "id": "47a8eb8d"
   },
   "source": [
    "### Instantiate the loss to cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a49ae3c9",
   "metadata": {
    "id": "a49ae3c9"
   },
   "outputs": [],
   "source": [
    "# use the cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a49045a",
   "metadata": {
    "id": "8a49045a"
   },
   "source": [
    "### Instantiate the optimizer to SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa3a04dd",
   "metadata": {
    "id": "aa3a04dd"
   },
   "outputs": [],
   "source": [
    "# use SGD .. use the momentum and weight decay vars\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ef11d",
   "metadata": {
    "id": "f93ef11d"
   },
   "source": [
    "#### Create the learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0e1727a",
   "metadata": {
    "id": "a0e1727a"
   },
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "metric = 0\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=4)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, verbose = True)\n",
    "scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "scheduler2 = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fe08caa",
   "metadata": {
    "id": "2fe08caa"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "#     transforms.Grayscale(3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean_RGB, imagenet_std_RGB),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236528e1",
   "metadata": {
    "id": "236528e1"
   },
   "source": [
    "### Create the train dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c29f6b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "2b3becae737f4a8abdd600ae5eff928c",
      "88a5c037f6794636af9695c36b6298a3",
      "473c668945fc4b5c86dbc773d8dfa76b",
      "af1448bbbf08412ebbc0b69a2f376b6c",
      "ab3278eec5dd479899e8d74031571e8f",
      "6fd1fcb1487045679aefac21fc12a1b2",
      "3cdbfeb8c727494cbb38cd182ab40d41",
      "276f114036af490cbd8e67fdfe9eed6f",
      "08affbf9ac3649adbc1321114263b65a",
      "2d6e3d5515a944888c2eaa4aaa963c98",
      "66b099ca3dbf4418bcbdde851812035e"
     ]
    },
    "id": "7c29f6b1",
    "outputId": "4db1c1b5-11ed-4429-818e-a28a5b434250"
   },
   "outputs": [],
   "source": [
    "# Training dataset from local file\n",
    "train_dataset = ImageFolder(root=TRAINDIR, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63dfe3c0",
   "metadata": {
    "id": "63dfe3c0"
   },
   "outputs": [],
   "source": [
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE2),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "#     transforms.Grayscale(3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean_RGB, std=imagenet_std_RGB),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca6c39",
   "metadata": {
    "id": "38ca6c39"
   },
   "source": [
    "### Create the val dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42d58f82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42d58f82",
    "outputId": "d837b8fe-317a-4d55-fd76-d46287f74edc"
   },
   "outputs": [],
   "source": [
    "#Validation dataset from local file\n",
    "val_dataset = ImageFolder(root=VALDIR,transform=transform_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a291660",
   "metadata": {
    "id": "3a291660"
   },
   "source": [
    "### Create the train dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "574373be",
   "metadata": {
    "id": "574373be"
   },
   "outputs": [],
   "source": [
    "# fill this in\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280c6e0",
   "metadata": {
    "id": "b280c6e0"
   },
   "source": [
    "### Create the validation dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6aa623fe",
   "metadata": {
    "id": "6aa623fe"
   },
   "outputs": [],
   "source": [
    "# fill this in..\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=WORKERS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cfa6766",
   "metadata": {
    "id": "7cfa6766"
   },
   "outputs": [],
   "source": [
    "best_acc1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d0620a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d0620a6",
    "outputId": "1d536dca-605e-4d04-e7cb-23a91430a640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][  0/216]\tTime 26.333 (26.333)\tData 20.421 (20.421)\tLoss 2.2691e+00 (2.2691e+00)\tAcc@1  19.00 ( 19.00)\tAcc@5  60.00 ( 60.00)\n",
      "Test: [ 0/27]\tTime 24.747 (24.747)\tLoss 1.2270e+00 (1.2270e+00)\tAcc@1  81.00 ( 81.00)\tAcc@5  99.00 ( 99.00)\n",
      " * Acc@1 85.370 Acc@5 99.815\n",
      "lr: [0.09000000000000001]\n",
      "Epoch: [1][  0/216]\tTime 21.289 (21.289)\tData 20.546 (20.546)\tLoss 1.3607e+00 (1.3607e+00)\tAcc@1  81.00 ( 81.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 0/27]\tTime 32.205 (32.205)\tLoss 7.4886e-01 (7.4886e-01)\tAcc@1  91.00 ( 91.00)\tAcc@5 100.00 (100.00)\n",
      " * Acc@1 87.185 Acc@5 99.815\n",
      "lr: [0.08100000000000002]\n",
      "Epoch: [2][  0/216]\tTime 23.374 (23.374)\tData 22.525 (22.525)\tLoss 1.3086e+00 (1.3086e+00)\tAcc@1  78.00 ( 78.00)\tAcc@5  99.00 ( 99.00)\n",
      "Test: [ 0/27]\tTime 27.919 (27.919)\tLoss 5.0864e-01 (5.0864e-01)\tAcc@1  91.00 ( 91.00)\tAcc@5  99.00 ( 99.00)\n",
      " * Acc@1 91.000 Acc@5 99.926\n",
      "lr: [0.07290000000000002]\n",
      "Epoch: [3][  0/216]\tTime 24.050 (24.050)\tData 23.158 (23.158)\tLoss 1.0695e+00 (1.0695e+00)\tAcc@1  80.00 ( 80.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 0/27]\tTime 28.439 (28.439)\tLoss 3.2524e-01 (3.2524e-01)\tAcc@1  94.00 ( 94.00)\tAcc@5 100.00 (100.00)\n",
      " * Acc@1 91.741 Acc@5 99.963\n",
      "lr: [0.06561000000000002]\n",
      "Epoch: [4][  0/216]\tTime 24.268 (24.268)\tData 23.341 (23.341)\tLoss 6.4263e-01 (6.4263e-01)\tAcc@1  90.00 ( 90.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 0/27]\tTime 28.047 (28.047)\tLoss 4.2307e-01 (4.2307e-01)\tAcc@1  91.00 ( 91.00)\tAcc@5 100.00 (100.00)\n",
      " * Acc@1 88.519 Acc@5 100.000\n",
      "lr: [0.05904900000000002]\n",
      "Epoch: [5][  0/216]\tTime 24.091 (24.091)\tData 23.317 (23.317)\tLoss 1.1837e+00 (1.1837e+00)\tAcc@1  81.00 ( 81.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 0/27]\tTime 24.799 (24.799)\tLoss 4.3408e-01 (4.3408e-01)\tAcc@1  91.00 ( 91.00)\tAcc@5 100.00 (100.00)\n",
      " * Acc@1 90.963 Acc@5 100.000\n",
      "lr: [0.05314410000000002]\n",
      "Epoch: [6][  0/216]\tTime 21.719 (21.719)\tData 20.906 (20.906)\tLoss 9.1721e-01 (9.1721e-01)\tAcc@1  79.00 ( 79.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 0/27]\tTime 25.419 (25.419)\tLoss 1.3272e-01 (1.3272e-01)\tAcc@1  98.00 ( 98.00)\tAcc@5 100.00 (100.00)\n",
      " * Acc@1 91.333 Acc@5 99.926\n",
      "lr: [0.04782969000000002]\n",
      "Epoch: [7][  0/216]\tTime 21.124 (21.124)\tData 20.314 (20.314)\tLoss 4.1898e-01 (4.1898e-01)\tAcc@1  87.00 ( 87.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 0/27]\tTime 27.935 (27.935)\tLoss 2.9575e-01 (2.9575e-01)\tAcc@1  92.00 ( 92.00)\tAcc@5 100.00 (100.00)\n",
      " * Acc@1 92.111 Acc@5 100.000\n",
      "lr: [0.043046721000000024]\n",
      "\n",
      "\n",
      " ****The best performing model has a Top 1 Accuracy of 92.11111450195312***\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "#    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion)\n",
    "\n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': ARCH,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "    scheduler1.step()\n",
    "    scheduler2.step()\n",
    "    print('lr: ' + str(scheduler2.get_last_lr()))\n",
    "    # break if top 1 accuracy greater than 60 or else will complete total number of epochs\n",
    "    if acc1>= 92:\n",
    "        break\n",
    "        \n",
    "# print out best performing model top 1 accuracy        \n",
    "print('\\n\\n ****The best performing model has a Top 1 Accuracy of {}***'.format(best_acc1))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cifar_lab_ECK.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08affbf9ac3649adbc1321114263b65a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "276f114036af490cbd8e67fdfe9eed6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2b3becae737f4a8abdd600ae5eff928c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_473c668945fc4b5c86dbc773d8dfa76b",
       "IPY_MODEL_af1448bbbf08412ebbc0b69a2f376b6c",
       "IPY_MODEL_ab3278eec5dd479899e8d74031571e8f"
      ],
      "layout": "IPY_MODEL_88a5c037f6794636af9695c36b6298a3"
     }
    },
    "2d6e3d5515a944888c2eaa4aaa963c98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3cdbfeb8c727494cbb38cd182ab40d41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "473c668945fc4b5c86dbc773d8dfa76b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cdbfeb8c727494cbb38cd182ab40d41",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6fd1fcb1487045679aefac21fc12a1b2",
      "value": ""
     }
    },
    "66b099ca3dbf4418bcbdde851812035e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fd1fcb1487045679aefac21fc12a1b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88a5c037f6794636af9695c36b6298a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab3278eec5dd479899e8d74031571e8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66b099ca3dbf4418bcbdde851812035e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2d6e3d5515a944888c2eaa4aaa963c98",
      "value": " 170499072/? [00:05&lt;00:00, 68144230.81it/s]"
     }
    },
    "af1448bbbf08412ebbc0b69a2f376b6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08affbf9ac3649adbc1321114263b65a",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_276f114036af490cbd8e67fdfe9eed6f",
      "value": 170498071
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
