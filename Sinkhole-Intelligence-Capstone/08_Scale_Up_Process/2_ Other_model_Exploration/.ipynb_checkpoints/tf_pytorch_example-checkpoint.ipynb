{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f16d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b567ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "# my script\n",
    "from w210_model_library import print_confusion_matrix, modelresults\n",
    "from w210_model_library import modelresults_2, crossvalidation,assignRisk, modelresults_3\n",
    "from w210_model_library import newPred, riskdistribution, fattrtype, importance_attr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# random.seed(1234)\n",
    "\n",
    "dirname = '../modeldata/'\n",
    "dirpm = '../prediction_train_test/'\n",
    "\n",
    "cols1 = ['Key','train_test','lon_t_x', 'lat_t', 'Group_x','label','Prediction','No_SH', 'SH', 'Num', 'name_x', 'DateD',\n",
    "        'imgnum','Sinkhole', 'ID',  'geometry', 'AnnualCrop', 'Forest',\n",
    "        'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential',\n",
    "        'River', 'SeaLake', 'prediction', 'prediction_name', 'l25', 'l50', 'l75', 'l100', 'l150', 'l200', \n",
    "        'l250', 'l300', 'l500', 'l750', 'l1000', 'l1000plus', 'coloc', 'Y25', 'Y50', 'Y75', 'Y100', 'Y150',\n",
    "        'Y200', 'Y250', 'Y300', 'Y500', 'Y750', 'Y1000', 'Y1000plus', 'Ycoloc', 'Key_ws', 'ws_name',\n",
    "        'lon_w', 'lat_w', 'County', 'Calcium Carbonate', 'Gypsum', 'Soil Health Organic Matter',\n",
    "        'Percent Clay', 'Percent Sand', 'Percent Silt', 'Available Water Storage', 'rolling_7_precip',\n",
    "        'rolling_15_precip', 'rolling_30_precip', 'rolling_60_precip', 'rolling_90_precip', 'y1_mean_prc',\n",
    "        'y1_max_prc', 'y1_mean_tmp', 'y1_max_tmp', 'y1_min_tmp', 'y2_mean_prc', 'y2_max_prc', 'y2_mean_tmp',\n",
    "        'y2_max_tmp', 'y2_min_tmp', 'y3_mean_prc', 'y3_max_prc', 'y3_mean_tmp', 'y3_max_tmp', 'y3_min_tmp',\n",
    "        'gridcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae12aea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finEvents = pd.read_csv(dirname+\"data_model1_365.csv\")\n",
    "finEvents.isnull().values.any(), finEvents.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f9cbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finEvents = finEvents.dropna()\n",
    "finEvents.isnull().values.any(), finEvents.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e627a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l200', 'Y250', 'l75', 'Y200', 'Y500', 'l150', 'l50', 'l500', 'Y300', 'l300', 'Y1000', 'l25', 'Y100', 'Y750', 'y2_mean_prc', 'l750', 'Y150', 'Y75', 'PermanentCrop', 'Highway', 'Percent Sand', 'Forest', 'Available Water Storage', 'Percent Silt', 'Industrial', 'Residential', 'Pasture', 'y1_max_tmp', 'rolling_60_precip', 'y3_max_tmp', 'HerbaceousVegetation', 'rolling_7_precip', 'y2_mean_tmp', 'l250']\n"
     ]
    }
   ],
   "source": [
    "dfvars = pd.read_csv(dirname+\"attr80.csv\")\n",
    "\n",
    "x_variables = list(dfvars[\"attribute\"].unique())\n",
    "len(x_variables)\n",
    "features = finEvents[x_variables]\n",
    "print(x_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7a9d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 0 44\n",
      "44 178 178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(110, 112)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X = np.array(features)\n",
    "Y = np.array(finEvents[\"Sinkhole\"])\n",
    "Group = np.array(finEvents[\"Group\"])\n",
    "Keys = np.array(finEvents[\"Key\"])\n",
    "Lon_t = np.array(finEvents[\"lon_t\"])\n",
    "Lat_t = np.array(finEvents[\"lat_t_x\"])\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "\n",
    "X, Y, Group = X[shuffle], Y[shuffle], Group[shuffle]\n",
    "Keys, Lon_t, Lat_t = Keys[shuffle], Lon_t[shuffle], Lat_t[shuffle]\n",
    "\n",
    "\n",
    "# Define sizes for train, development and test data (0.8, 0.0, 0.2)\n",
    "train = 0.80\n",
    "val = 0\n",
    "test = 1- train\n",
    "\n",
    "num_images = len(Y)\n",
    "train_size = int(round(num_images * train,0))\n",
    "val_size = int(round(num_images * val,0))\n",
    "test_size = num_images - train_size - val_size\n",
    "\n",
    "print(train_size, val_size, test_size)\n",
    "\n",
    "test_data, test_labels, test_group = X[train_size+val_size:], Y[train_size+val_size:], Group[train_size+val_size:]\n",
    "test_keys, test_lont_t, test_lat_t = Keys[train_size+val_size:],  Lon_t[train_size+val_size:], Lat_t[train_size+val_size:]\n",
    "\n",
    "# val_data, val_labels = X[train_size:train_size+val_size], Y[train_size:train_size+val_size]\n",
    "train_data, train_labels, train_group = X[:train_size], Y[:train_size], Group[:train_size]\n",
    "train_keys, train_lon_t, train_lat_t = Keys[:train_size],  Lon_t[:train_size], Lat_t[:train_size]\n",
    "\n",
    "dftrain = pd.DataFrame({\"Key\": train_keys, \"lon_t\": train_lon_t, \"lat_t\": train_lat_t, \"Group\": train_group})\n",
    "dftest = pd.DataFrame({\"Key\": test_keys, \"lon_t\": test_lont_t, \"lat_t\":test_lat_t, \"Group\": test_group })\n",
    "\n",
    "print(len(test_data), len(train_data), len(train_group))\n",
    "\n",
    "np.count_nonzero(Y == 0), np.count_nonzero(Y == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512c5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc981e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8984, 0.8879, 0.1025],\n",
      "        [0.2607, 0.6284, 0.8523],\n",
      "        [0.2174, 0.7821, 0.6938],\n",
      "        [0.7046, 0.3197, 0.7145],\n",
      "        [0.4090, 0.1850, 0.2388]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc67f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w210tf",
   "language": "python",
   "name": "w210tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
