{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23593645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "# my script\n",
    "from w210_model_library import print_confusion_matrix, modelresults\n",
    "from w210_model_library import modelresults_2, crossvalidation,assignRisk, modelresults_3\n",
    "from w210_model_library import newPred, riskdistribution, fattrtype, importance_attr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# random.seed(1234)\n",
    "\n",
    "dirname = '../modeldata/'\n",
    "dirpm = '../prediction_train_test/'\n",
    "\n",
    "cols1 = ['Key','train_test','lon_t_x', 'lat_t', 'Group_x','label','Prediction','No_SH', 'SH', 'Num', 'name_x', 'DateD',\n",
    "        'imgnum','Sinkhole', 'ID',  'geometry', 'AnnualCrop', 'Forest',\n",
    "        'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential',\n",
    "        'River', 'SeaLake', 'prediction', 'prediction_name', 'l25', 'l50', 'l75', 'l100', 'l150', 'l200', \n",
    "        'l250', 'l300', 'l500', 'l750', 'l1000', 'l1000plus', 'coloc', 'Y25', 'Y50', 'Y75', 'Y100', 'Y150',\n",
    "        'Y200', 'Y250', 'Y300', 'Y500', 'Y750', 'Y1000', 'Y1000plus', 'Ycoloc', 'Key_ws', 'ws_name',\n",
    "        'lon_w', 'lat_w', 'County', 'Calcium Carbonate', 'Gypsum', 'Soil Health Organic Matter',\n",
    "        'Percent Clay', 'Percent Sand', 'Percent Silt', 'Available Water Storage', 'rolling_7_precip',\n",
    "        'rolling_15_precip', 'rolling_30_precip', 'rolling_60_precip', 'rolling_90_precip', 'y1_mean_prc',\n",
    "        'y1_max_prc', 'y1_mean_tmp', 'y1_max_tmp', 'y1_min_tmp', 'y2_mean_prc', 'y2_max_prc', 'y2_mean_tmp',\n",
    "        'y2_max_tmp', 'y2_min_tmp', 'y3_mean_prc', 'y3_max_prc', 'y3_mean_tmp', 'y3_max_tmp', 'y3_min_tmp',\n",
    "        'gridcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a924d5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finEvents = pd.read_csv(dirname+\"data_model1_365.csv\")\n",
    "finEvents.isnull().values.any(), finEvents.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05141014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finEvents = finEvents.dropna()\n",
    "finEvents.isnull().values.any(), finEvents.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10846eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l200', 'Y250', 'l75', 'Y200', 'Y500', 'l150', 'l50', 'l500', 'Y300', 'l300', 'Y1000', 'l25', 'Y100', 'Y750', 'y2_mean_prc', 'l750', 'Y150', 'Y75', 'PermanentCrop', 'Highway', 'Percent Sand', 'Forest', 'Available Water Storage', 'Percent Silt', 'Industrial', 'Residential', 'Pasture', 'y1_max_tmp', 'rolling_60_precip', 'y3_max_tmp', 'HerbaceousVegetation', 'rolling_7_precip', 'y2_mean_tmp', 'l250']\n"
     ]
    }
   ],
   "source": [
    "dfvars = pd.read_csv(dirname+\"attr80.csv\")\n",
    "\n",
    "x_variables = list(dfvars[\"attribute\"].unique())\n",
    "len(x_variables)\n",
    "features = finEvents[x_variables]\n",
    "print(x_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4978d51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 0 44\n",
      "44 178 178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(110, 112)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X = np.array(features)\n",
    "Y = np.array(finEvents[\"Sinkhole\"])\n",
    "Group = np.array(finEvents[\"Group\"])\n",
    "Keys = np.array(finEvents[\"Key\"])\n",
    "Lon_t = np.array(finEvents[\"lon_t\"])\n",
    "Lat_t = np.array(finEvents[\"lat_t_x\"])\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "\n",
    "X, Y, Group = X[shuffle], Y[shuffle], Group[shuffle]\n",
    "Keys, Lon_t, Lat_t = Keys[shuffle], Lon_t[shuffle], Lat_t[shuffle]\n",
    "\n",
    "\n",
    "# Define sizes for train, development and test data (0.8, 0.0, 0.2)\n",
    "train = 0.80\n",
    "val = 0\n",
    "test = 1- train\n",
    "\n",
    "num_images = len(Y)\n",
    "train_size = int(round(num_images * train,0))\n",
    "val_size = int(round(num_images * val,0))\n",
    "test_size = num_images - train_size - val_size\n",
    "\n",
    "print(train_size, val_size, test_size)\n",
    "\n",
    "test_data, test_labels, test_group = X[train_size+val_size:], Y[train_size+val_size:], Group[train_size+val_size:]\n",
    "test_keys, test_lont_t, test_lat_t = Keys[train_size+val_size:],  Lon_t[train_size+val_size:], Lat_t[train_size+val_size:]\n",
    "\n",
    "# val_data, val_labels = X[train_size:train_size+val_size], Y[train_size:train_size+val_size]\n",
    "train_data, train_labels, train_group = X[:train_size], Y[:train_size], Group[:train_size]\n",
    "train_keys, train_lon_t, train_lat_t = Keys[:train_size],  Lon_t[:train_size], Lat_t[:train_size]\n",
    "\n",
    "dftrain = pd.DataFrame({\"Key\": train_keys, \"lon_t\": train_lon_t, \"lat_t\": train_lat_t, \"Group\": train_group})\n",
    "dftest = pd.DataFrame({\"Key\": test_keys, \"lon_t\": test_lont_t, \"lat_t\":test_lat_t, \"Group\": test_group })\n",
    "\n",
    "print(len(test_data), len(train_data), len(train_group))\n",
    "\n",
    "np.count_nonzero(Y == 0), np.count_nonzero(Y == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ecdfd1",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e22b6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#sc?ler = 'none'\n",
    "model = BernoulliNB()\n",
    "\n",
    "alpha_list = [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 20, 30 ,20 ,30 ,100]\n",
    "binarize = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "parameters = [{'mod__alpha': alpha_list,\n",
    "               'mod__binarize': binarize}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db51b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bestmodel.predict(test_data))\n",
    "# print(bestmodel.get_params())\n",
    "# print(bestmodel.predict_proba(test_data))\n",
    "# print(bestmodel.predict_log_proba(test_data))\n",
    "np.log(8.66396846e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21537709",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73329d4",
   "metadata": {},
   "source": [
    "## Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcff1f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "scaler = 'none'\n",
    "model = MultinomialNB()\n",
    "\n",
    "alpha_list = [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 20, 30 ,20 ,30 ,100]\n",
    "\n",
    "parameters = [{'mod__alpha': alpha_list}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "# alpha = cv1.best_estimator_.get_params()['mod__alpha']\n",
    "# binarize = cv1.best_estimator_.get_params()['mod__binarize']\n",
    "\n",
    "# print(\"alpha: \",alpha, \" binarize\", binarize)\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13adcaed",
   "metadata": {},
   "source": [
    "## Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "scaler = 'none'\n",
    "model = GaussianNB()\n",
    "\n",
    "smoothing_list = [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]\n",
    "\n",
    "parameters = [{'mod__var_smoothing': smoothing_list}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "# alpha = cv1.best_estimator_.get_params()['mod__alpha']\n",
    "# binarize = cv1.best_estimator_.get_params()['mod__binarize']\n",
    "\n",
    "# print(\"alpha: \",alpha, \" binarize\", binarize)\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ba7fc",
   "metadata": {},
   "source": [
    "## KNN - Nearest Neighbors\n",
    "\n",
    "- The default metric is minkowski,\n",
    "- algorithm defaul is auto.\n",
    "- weights defaul is uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e66625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#sc?ler = 'none'\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "metric_list = ['euclidean', 'manhattan', 'chebyshev', 'minkowski', 'wminkowski', 'seuclidean', 'mahalanobis']\n",
    "weights = [\"uniform\", \"distance\"]\n",
    "algorithm_list = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "k_list = [1, 2, 3, 4, 5, 6, 7, 8 , 9 , 10, 11, 12, 13, 14, 15, 20, 25]\n",
    "\n",
    "parameters = [{\n",
    "#                'mod__metric': metric_list,\n",
    "#                'mod__weights': weights,\n",
    "#                'mod__algorithm': algorithm_list,\n",
    "               'mod__n_neighbors':k_list}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8dbdf1",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "scaler = 'none'\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# min_samples_split=10\n",
    "\n",
    "criterion_list = [\"entropy\", \"gini\"]\n",
    "max_depth_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "min_samples_leaf_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "parameters = [{'mod__max_depth': max_depth_list,\n",
    "               'mod__min_samples_leaf': min_samples_leaf_list,\n",
    "               'mod__criterion':criterion_list}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ecb962",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "scaler = 'none'\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "parameters = [{'mod__n_estimators': [20, 30, 40, 50, 60, 70, 80],\n",
    "               'mod__max_depth': [2, 3, 4]}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af4fba3",
   "metadata": {},
   "source": [
    "## ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaler = 'none'\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "algorithm_list = [\"SAMME\", \"SAMME.R\"]\n",
    "n_estimators_list = [5, 10, 15, 20, 25]\n",
    "learning_rate_list = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.55, 0.6]\n",
    "\n",
    "parameters = [{'mod__algorithm': algorithm_list,\n",
    "               'mod__learning_rate': learning_rate_list,\n",
    "               'mod__n_estimators': n_estimators_list}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528f07e",
   "metadata": {},
   "source": [
    "## SVM - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1494d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaler = 'none'\n",
    "model = svm.LinearSVC()\n",
    "\n",
    "c_list = [0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 10, 20]\n",
    "max_iter_list = [20000]\n",
    "\n",
    "parameters = [{'mod__C': c_list,\n",
    "               'mod__max_iter': max_iter_list}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398e82f",
   "metadata": {},
   "source": [
    "## SVM - Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaler = 'none'\n",
    "model = svm.SVC(kernel='poly')\n",
    "\n",
    "c_list = [0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 10, 20]\n",
    "max_iter_list = [1000, 2000, 5000, 8000, 10000]\n",
    "degree_list = [1, 2, 3, 4, 5]\n",
    "gamma_list = [0.1, 0.2, 0.5, 0.7, 0.8]\n",
    "\n",
    "parameters = [{'mod__C': c_list,\n",
    "               'mod__degree': degree_list,\n",
    "               'mod__gamma': gamma_list,\n",
    "               'mod__max_iter': max_iter_list}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00ab7e",
   "metadata": {},
   "source": [
    "## SVM - RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a20ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaler = 'none'\n",
    "model = svm.SVC(kernel='rbf')\n",
    "\n",
    "c_list = [0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 10, 20]\n",
    "max_iter_list = [1000, 2000, 5000, 8000, 10000]\n",
    "gamma_list = [0.1, 0.2, 0.5, 0.7, 0.8]\n",
    "\n",
    "parameters = [{'mod__C': c_list,\n",
    "               'mod__gamma': gamma_list,\n",
    "               'mod__max_iter': max_iter_list}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246b5e9",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc47a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaler = 'none'\n",
    "model = MLPClassifier()\n",
    "\n",
    "activation_list = [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
    "max_iter_list = [1000, 2000, 5000]\n",
    "layer_list = [(10,10,10), (5,5,5), (3,3,3), (20, 20, 20)]\n",
    "learning_rate_init_list = [0.003, 0.02, 0.01, 0.1, 0.2, 0.3, 0.5]\n",
    "learning_rate_list = ['constant', 'invscaling', 'adaptive']\n",
    "\n",
    "solver_list = [\"lbfgs\", \"sgd\", \"adam\"]\n",
    "alpha_list = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.5]\n",
    "\n",
    "# MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=8000, activation=activation, solver=solver)\n",
    "\n",
    "parameters = [{'mod__hidden_layer_sizes': layer_list,\n",
    "               'mod__activation': activation_list,\n",
    "               'mod__learning_rate_init': learning_rate_init_list,\n",
    "               'mod__learning_rate': learning_rate_list,\n",
    "               'mod__solver': solver_list,\n",
    "               'mod__max_iter': max_iter_list}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a6c2b",
   "metadata": {},
   "source": [
    "## MLPC - Using alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaler = 'none'\n",
    "model = MLPClassifier()\n",
    "\n",
    "activation_list = [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
    "max_iter_list = [1000, 2000, 5000]\n",
    "layer_list = [(10,10,10), (5,5,5), (3,3,3), (20, 20, 20)]\n",
    "learning_rate_init_list = [0.003, 0.02, 0.01, 0.1, 0.2, 0.3, 0.5]\n",
    "learning_rate_list = ['constant', 'invscaling', 'adaptive']\n",
    "\n",
    "solver_list = [\"lbfgs\", \"sgd\", \"adam\"]\n",
    "alpha_list = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.5]\n",
    "\n",
    "# MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=8000, activation=activation, solver=solver)\n",
    "\n",
    "parameters = [{'mod__solver': solver_list,\n",
    "               'mod__hidden_layer_sizes': layer_list,\n",
    "               'mod__alpha': alpha_list,\n",
    "               'mod__activation': alpha_list,\n",
    "               'mod__learning_rate_init': learning_rate_init_list,\n",
    "               'mod__learning_rate': learning_rate_list,\n",
    "               'mod__max_iter': max_iter_list}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b02f3",
   "metadata": {},
   "source": [
    "## PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c8f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Helper function to show line of cummulative variance explained by each pca\n",
    "def varplot2(exp_var_ratio):\n",
    "    \n",
    "    cum_var_exp= np.cumsum(exp_var_ratio)\n",
    "\n",
    "    with plt.style.context('seaborn-whitegrid'):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        #plt.step(range(len(exp_var_ratio)), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "        plt.plot(range(len(exp_var_ratio)), cum_var_exp, label='cumulative explained variance')\n",
    "        plt.ylabel('Explained variance ratio')\n",
    "        plt.xlabel('Principal components')\n",
    "        plt.legend(loc='best')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34afcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_pca = PCA()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pca = fp_pca.fit_transform(train_data)\n",
    "\n",
    "df_summary = pd.DataFrame({\"Dimension\": range(1,fp_pca.explained_variance_ratio_.shape[0]+1),\n",
    "                           \"Var Prop Exp\": fp_pca.explained_variance_ratio_,\n",
    "                            \"Var Cum Prop Exp (%)\": np.cumsum(fp_pca.explained_variance_ratio_)})\n",
    "#varplot1(p1_pca.explained_variance_)\n",
    "varplot2(fp_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e4d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(fp_pca.explained_variance_ratio_)[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97078c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [0, 1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 60]\n",
    "\n",
    "df_summary = pd.DataFrame({\"Dimension\": range(1,fp_pca.explained_variance_ratio_.shape[0]+1),\n",
    "                          \"Var Prop Exp\": fp_pca.explained_variance_ratio_,\n",
    "                          \"Var Cum Prop Exp (%)\": np.cumsum(fp_pca.explained_variance_ratio_)})\n",
    "\n",
    "#df_summary[\"Cum 2\"] = df_summary['Var Prop Exp'].cumsum()\n",
    "\n",
    "print(df_summary[df_summary['Dimension'].isin(k_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c65f5c",
   "metadata": {},
   "source": [
    "## Selecting Top PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE THIS CODE WHEN SELECTING TOP N PRINCIPAL COMPONENTS FOR THE MODELS ONLY\n",
    "n_components = 20\n",
    "pca_final = PCA(n_components=n_components)\n",
    "train_data = pca_final.fit_transform(train_data)\n",
    "test_data = pca_final.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eccbc0",
   "metadata": {},
   "source": [
    "## KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(train_data)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaler = 'none'\n",
    "model = KMeans()\n",
    "\n",
    "n_clusters_list = [2]\n",
    "max_iter_list = [10, 20, 100, 200, 500, 800, 1000, 2000, 5000, 8000, 10000]\n",
    "algorithm_list = ['lloyd', 'elkan', 'auto', 'full']\n",
    "\n",
    "parameters = [{'mod__n_clusters': n_clusters_list,\n",
    "               'mod__algorithm': algorithm_list,\n",
    "               'mod__max_iter': max_iter_list}]\n",
    "\n",
    "cv1 = crossvalidation(scaler, model, parameters, train_data, train_labels, 10, 'f1')\n",
    "\n",
    "print(cv1.best_estimator_.get_params())\n",
    "\n",
    "bestmodel = cv1.best_estimator_\n",
    "print(bestmodel)\n",
    "\n",
    "dfres, dfpre, pipeline = modelresults_3(train_data, train_labels, \n",
    "                                      test_data, test_labels, bestmodel)\n",
    "\n",
    "\n",
    "print_confusion_matrix(test_labels, dfpre[\"Test\"], \"Confusion Matrix - Test Data\")\n",
    "print_confusion_matrix(train_labels, dfpre['Train'], \"Confusion Matrix - Train Data\")\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f2fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w210",
   "language": "python",
   "name": "w210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
